Data loading 

we initially tried connecting with ec2 
ec2 connected to s3 in 2 ways 1) using credentials 2) using roles

later we replaced ec2 with the snowflake
snowflake connected to aws account ie s3 in 2 ways 
1) credenetials
2) storage integration

In storage integration we are giving a file format which consists of all the details file type, pre delimiter etc..

so we have pushed all of this into a stage.
created a stage and used it in the COPY command, so that it looks simple for us.

Interal Stage 
1) User's stage  2) Table stage and  3) Internal named stage = > Greatest degree of flexibilty. Multiple users can share and multiple table cdn access.
spific to users      specific to tables      
					file in the (Table stage) can 
					-accessed by multiple users but not multiple table.
					vice versa for Users stage.
Lec - 6
Validation Errors; how to handle them 
create or replace table emp (
    empid number(38, 0),
    empname varchar(100),
    loc varchar(100)  
);

select * from emp;

-- -- Validation Mode

select * from 
table(information_schema.copy_history(table_name=>'EMP', start_time=>dateadd(hours, -6, current_timestamp())))

-- // VALIDATION_MODE = 'RETURN_ERRORS'

copy into emp 
from s3://nvt/snowflake/Raw_data/Emp/emp_data_errors.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV)
VALIDATION_MODE = 'RETURN_ERRORS' -- will encounter the errors before loading/ (BEFORE PROCESSING) the data into file.

-- // ON_ERROR = ABORT_STATEMENT (BY DEFAULT) if we get error it will abort and show the error

-- // ON_ERROR = CONTINUE 

copy into emp 
from s3://nvt/snowflake/Raw_data/Emp/emp_data_errors.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV)
ON_ERROR = CONTINUE;

-- select * from emp;

-- // after giving ON_ERROR = CONTINUE, To validate errors: job_id = query id

select * from table(validate(EMP, job_id => '01a88d12-3200-99ed-0002-713600039162')); -- showing where it went wrong...

-- VALIDATION_MODE = 'RETURN_ALL_ERRORS'

copy into emp 
from s3://nvt/snowflake/Raw_data/Emp/emp_data_errors.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV)
VALIDATION_MODE = 'RETURN_ALL_ERRORS';  -- RETURNS ERRORS FOR BOTH PROCESSED AND UNPROCESSED FILES. 
 

-- error file skip

COPY INTO EMP
FROM S3://nvt/snowflake/Raw_data/Emp/
storage_integration = AWS_S3_SOURCE_DATA
ON_ERROR = 'SKIP_FILE';


// WILL GET AN ERROR BELOW: SOLUTION IN INCLUDE THE DOUBLE QUOTES.. SEE FILE EMP_ENCLOSED. 

COPY INTO EMP
FROM S3://nvt/snowflake/Raw_data/Emp/emp_delimiter_error.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE = CSV);


// AFTER WHICH TYPE BELOW 

COPY INTO EMP
FROM S3://nvt/snowflake/Raw_data/Emp/emp_enclosed.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='"');

-- // MULTILINE EXAMPLE - SEE emp_multiline.txt SOLUTION: SAME AS ENCLOSED IN DOUBLE "" QUOTES IN DATA
SELECT * FROM EMP;


COPY INTO EMP
FROM S3://nvt/snowflake/Raw_data/Emp/emp_multiline.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='"');

-- // null example

COPY INTO EMP
FROM s3://nvt/snowflake/Raw_data/Emp/null_if_emp.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV NULL_IF=('NULL',''))
FORCE = TRUE; -- NOTE IT

-- // PIPE SEPARATOR

COPY INTO EMP
FROM s3://nvt/snowflake/Raw_data/Emp/emp_pipe_sep_file.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV FIELD_DELIMITER = '|');

-- // Purge option data to be deleted once it is loaded into the file / table


COPY INTO EMP
FROM s3://nvt/snowflake/Raw_data/Emp/emp_enclosed.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = (TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"')
PURGE=TRUE
FORCE = TRUE;

-- // FILE FORMAT OBJECT create an object which is most common

CREATE FILE FORMAT F1 
type = csv
field_optionally_enclosed_by = '"'


COPY INTO EMP
FROM s3://nvt/snowflake/Raw_data/Emp/emp_enclosed.txt
storage_integration = AWS_S3_SOURCE_DATA
FILE_FORMAT = F1
force = true;


-- // Load with Transformation (we want small transformation in the COPY command itself).

create or replace table emp_raw(

    fname varchar(100),
    frownumber varchar(100),
    empid integer,
    empname varchar(100),
    loc  varchar(100),
    load_ts timestamp_ntz
);


COPY INTO EMP_RAW
FROM (select metadata$filename, metadata$file_row_number, stg.$1, stg.$2, stg.$3, current_timestamp :: timestamp_ntz from 
     @stg_s3_source_data/Emp/ stg)
FILE_FORMAT = (TYPE=CSV)
ON_ERROR=CONTINUE;

-- ==> metadata$filename will give the file name present in the s3
-- ==> stg$1 will give columns in the stage
 
-- DELETE FROM EMP;
SELECT * FROM EMP_RAW;

